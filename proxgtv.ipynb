{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as ss\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 ms ± 5.06 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "F = np.random.rand(32*32)\n",
    "X = np.random.rand(32*32)\n",
    "width = 32\n",
    "if type(F) != torch.Tensor:\n",
    "        F = torch.from_numpy(F)\n",
    "        X = torch.from_numpy(X)\n",
    "F = F.unsqueeze(0).unsqueeze(0)\n",
    "X = X.unsqueeze(0).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    pixel_indices = [i for i in range(width * width)]\n",
    "    pixel_indices = np.reshape(pixel_indices, (width, width))\n",
    "    A = connected_adjacency(pixel_indices, connect='8')\n",
    "    A_pair = np.asarray(np.where(A.toarray() == 1)).T\n",
    "\n",
    "    def lambda_func(x):\n",
    "        return get_w(x, F)\n",
    "\n",
    "    W = list(map(lambda_func, A_pair))\n",
    "    A = torch.zeros(F.shape[0], width ** 2, width ** 2).type(dtype)\n",
    "    R = X.repeat(1, X.shape[2], 1).type(dtype)\n",
    "    R = abs(R - R.permute(0, 2, 1) )\n",
    "\n",
    "    for idx, p in enumerate(A_pair):\n",
    "        # CAN SPEED UP THIS\n",
    "        i = p[0]\n",
    "        j = p[1]\n",
    "        A[:, i, j] = W[idx]\n",
    "    GTV = (R*A).sum()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "GTV 2.26 ms ± 34.9 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
    "GTV1 459 ms ± 5.45 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def prox_gtv(w, v, eta=1):\n",
    "    \n",
    "    v[w>0] = v[w>0] - eta * w[w>0] * torch.sign(v[w>0])\n",
    "    v[w<=0] = 0\n",
    "#     if w>0:\n",
    "#         if abs(v) <= abs(eta*w):\n",
    "#             return 0\n",
    "#         else:\n",
    "#             return v - eta * w * np.sign(v)\n",
    "#     elif w<0:\n",
    "#         if abs(v) >= abs(eta*w):\n",
    "#             return 0\n",
    "#         else:\n",
    "#             return v - eta * w * np.sign(v)\n",
    "#     else:\n",
    "#         return 0\n",
    "    return v\n",
    "    \n",
    "def proximal_gradient_descent(x, y, w=0.1, eta=1): \n",
    "    grad = eta * (2*x - 2*y)\n",
    "    v = x - grad\n",
    "    x = prox_gtv(w, v, eta)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = list()\n",
    "rg = list()\n",
    "idx = list()\n",
    "for i in range(1000):\n",
    "    d = (i - 500) / 500\n",
    "    idx.append(d)\n",
    "    eta=1\n",
    "    w1 = 0.5 \n",
    "    w2 = -0.5\n",
    "    g.append(prox_gtv(w1, d, eta))\n",
    "    rg.append(prox_gtv(w2, d, eta))\n",
    "#     rg.append(np.exp(-(d*d)/(0.1**2))*d)\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "plt.gca()\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.scatter(idx, g, label='w1', s=0.75)\n",
    "ax.scatter(idx, rg, label='w2', s=0.75)\n",
    "ax.legend()\n",
    "ax.get_yaxis().set_visible(False)\n",
    "# ax.get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([24.9750], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f465bbfb090>]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = 25\n",
    "steps = [x]\n",
    "T = 20\n",
    "a = np.array([25.0])\n",
    "b = np.array([.05])\n",
    "y = np.array([25.0])\n",
    "w = torch.from_numpy(b)\n",
    "v = torch.from_numpy(a)\n",
    "y = torch.from_numpy(y)\n",
    "for i in range(T):\n",
    "    x = proximal_gradient_descent(x=v, y=y, w=w, eta=.5)\n",
    "    steps.append(x)\n",
    "print(x)\n",
    "plt.plot(list(range(T+1)), steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "if cuda:\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "\n",
    "class cnnf(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN F of GLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(cnnf, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.layer2a = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1), nn.ReLU()\n",
    "        )\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layer3a = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.ReLU()\n",
    "        )\n",
    "        # self.maxpool\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1), nn.ReLU()\n",
    "        )\n",
    "        # DECONVO\n",
    "\n",
    "        self.deconvo1 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # CONCAT with output of layer2\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # DECONVO\n",
    "        self.deconvo2 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "        # CONCAT with output of layer1\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outl1 = self.layer1(x)\n",
    "        outl2 = self.layer2a(outl1)\n",
    "        outl2 = self.maxpool(outl2)\n",
    "        outl2 = self.layer2(outl2)\n",
    "        outl3 = self.layer3a(outl2)\n",
    "        outl3 = self.maxpool(outl3)\n",
    "        outl3 = self.layer3(outl3)\n",
    "        outl3 = self.deconvo1(outl3)\n",
    "        outl3 = torch.cat((outl3, outl2), dim=1)\n",
    "        outl4 = self.layer4(outl3)\n",
    "        outl4 = self.deconvo2(outl4)\n",
    "        outl4 = torch.cat((outl4, outl1), dim=1)\n",
    "        del outl1, outl2, outl3\n",
    "        out = self.layer5(outl4)\n",
    "        return out\n",
    "\n",
    "class cnny(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Y of GLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(cnny, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.layer(x)\n",
    "        out = identity + out\n",
    "        del identity\n",
    "        return out\n",
    "\n",
    "class cnnp(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN Y of GLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, wt=36):\n",
    "        super(cnnp, self).__init__()\n",
    "        self.wt = wt\n",
    "        self.layer = nn.Sequential(\n",
    "            torch.nn.Linear(self.wt**2, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(100, 1),\n",
    "            torch.nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer(x)\n",
    "        return out\n",
    "    \n",
    "class RENOIR_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset loader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.npath = os.path.join(img_dir, \"noisy\")\n",
    "        self.rpath = os.path.join(img_dir, \"ref\")\n",
    "        self.nimg_name = sorted(os.listdir(self.npath))\n",
    "        self.rimg_name = sorted(os.listdir(self.rpath))\n",
    "        self.nimg_name = [\n",
    "            i\n",
    "            for i in self.nimg_name\n",
    "            if i.split(\".\")[-1].lower() in [\"jpeg\", \"jpg\", \"png\", \"bmp\"]\n",
    "        ]\n",
    "        self.rimg_name = [\n",
    "            i\n",
    "            for i in self.rimg_name\n",
    "            if i.split(\".\")[-1].lower() in [\"jpeg\", \"jpg\", \"png\", \"bmp\"]\n",
    "        ]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nimg_name)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        nimg_name = os.path.join(self.npath, self.nimg_name[idx])\n",
    "        nimg = cv2.imread(nimg_name)\n",
    "        rimg_name = os.path.join(self.rpath, self.rimg_name[idx])\n",
    "        rimg = cv2.imread(rimg_name)\n",
    "\n",
    "        sample = {\"nimg\": nimg, \"rimg\": rimg}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class standardize(object):\n",
    "    \"\"\"Convert opencv BGR to RGB order. Scale the image with a ratio\"\"\"\n",
    "\n",
    "    def __init__(self, scale=None, w=None, normalize=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        scale (float): resize height and width of samples to scale*width and scale*height\n",
    "        width (float): resize height and width of samples to width x width. Only works if \"scale\" is not specified\n",
    "        \"\"\"\n",
    "        self.scale = scale\n",
    "        self.w = w\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        nimg, rimg = sample[\"nimg\"], sample[\"rimg\"]\n",
    "        if self.scale:\n",
    "            nimg = cv2.resize(nimg, (0, 0), fx=self.scale, fy=self.scale)\n",
    "            rimg = cv2.resize(rimg, (0, 0), fx=self.scale, fy=self.scale)\n",
    "        else:\n",
    "            if self.w:\n",
    "                nimg = cv2.resize(nimg, (self.w, self.w))\n",
    "                rimg = cv2.resize(rimg, (self.w, self.w))\n",
    "        if self.normalize:\n",
    "            nimg = cv2.resize(nimg, (0, 0), fx=1, fy=1)\n",
    "            rimg = cv2.resize(rimg, (0, 0), fx=1, fy=1)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_BGR2RGB)\n",
    "        rimg = cv2.cvtColor(rimg, cv2.COLOR_BGR2RGB)\n",
    "        if self.normalize:\n",
    "            nimg = nimg / 255\n",
    "            rimg = rimg / 255\n",
    "        return {\"nimg\": nimg, \"rimg\": rimg}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \"\"\"\n",
    "        Swap color axis from H x W x C (numpy) to C x H x W (torch)\n",
    "        \"\"\"\n",
    "        nimg, rimg = sample[\"nimg\"], sample[\"rimg\"]\n",
    "        nimg = nimg.transpose((2, 0, 1))\n",
    "        rimg = rimg.transpose((2, 0, 1))\n",
    "        return {\"nimg\": torch.from_numpy(nimg), \"rimg\": torch.from_numpy(rimg)}\n",
    "\n",
    "\n",
    "def connected_adjacency(image, connect=8, patch_size=(1, 1)):\n",
    "    \"\"\"\n",
    "    Construct 8-connected pixels base graph (0 for not connected, 1 for connected)\n",
    "    \"\"\"\n",
    "    r, c = image.shape[:2]\n",
    "    r = int(r / patch_size[0])\n",
    "    c = int(c / patch_size[1])\n",
    "\n",
    "    if connect == \"4\":\n",
    "        # constructed from 2 diagonals above the main diagonal\n",
    "        d1 = np.tile(np.append(np.ones(c - 1), [0]), r)[:-1]\n",
    "        d2 = np.ones(c * (r - 1))\n",
    "        upper_diags = ss.diags([d1, d2], [1, c])\n",
    "        return upper_diags + upper_diags.T\n",
    "\n",
    "    elif connect == \"8\":\n",
    "        # constructed from 4 diagonals above the main diagonal\n",
    "        d1 = np.tile(np.append(np.ones(c - 1), [0]), r)[:-1]\n",
    "        d2 = np.append([0], d1[: c * (r - 1)])\n",
    "        d3 = np.ones(c * (r - 1))\n",
    "        d4 = d2[1:-1]\n",
    "        upper_diags = ss.diags([d1, d2, d3, d4], [1, c - 1, c, c + 1])\n",
    "        return upper_diags + upper_diags.T\n",
    "\n",
    "\n",
    "def get_w(ij, F):\n",
    "    \"\"\"\n",
    "    Compute weights for node i and node j using exemplars F\n",
    "    \"\"\"\n",
    "    fi, fj = F[:, :, ij[0]], F[:, :, ij[1]]\n",
    "    d = dist(fi, fj)\n",
    "    return w(d).type(dtype)\n",
    "\n",
    "\n",
    "def w(d, epsilon=1):\n",
    "    \"\"\"\n",
    "    Compute (3)\n",
    "    \"\"\"\n",
    "    return torch.exp(-d / (2 * epsilon ** 2))\n",
    "\n",
    "\n",
    "def dist(fi, fj):\n",
    "    \"\"\"\n",
    "    Compute the distance using equation (4)\n",
    "    \"\"\"\n",
    "    return torch.sum((fi - fj) ** 2, axis=1).type(dtype)\n",
    "\n",
    "\n",
    "def laplacian_construction(width, F, ntype=\"8\"):\n",
    "    \"\"\"\n",
    "    Construct Laplacian matrix\n",
    "    \"\"\"\n",
    "    if type(F) != torch.Tensor:\n",
    "        F = torch.from_numpy(F)\n",
    "    with torch.no_grad():\n",
    "        pixel_indices = [i for i in range(width * width)]\n",
    "        pixel_indices = np.reshape(pixel_indices, (width, width))\n",
    "        A = connected_adjacency(pixel_indices, connect='8')\n",
    "        A_pair = np.asarray(np.where(A.toarray() == 1)).T\n",
    "\n",
    "        def lambda_func(x):\n",
    "            return get_w(x, F)\n",
    "\n",
    "        W = list(map(lambda_func, A_pair))\n",
    "        A = torch.zeros(F.shape[0], width ** 2, width ** 2).type(dtype)\n",
    "#         R = X.repeat(1, X.shape[2], 1).type(dtype)\n",
    "#         R = abs(R - R.permute(0, 2, 1) )\n",
    "\n",
    "        for idx, p in enumerate(A_pair):\n",
    "            # CAN SPEED UP THIS\n",
    "            i = p[0]\n",
    "            j = p[1]\n",
    "            A[:, i, j] = W[idx]\n",
    "#         print(R.shape, A.shape)\n",
    "#         GTV = (R*A).sum()\n",
    "    return A\n",
    "#     return R.type(dtype)\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    \"\"\"\n",
    "    Initialize weights of convolutional layers\n",
    "    \"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n",
    "class GTV(nn.Module):\n",
    "    \"\"\"\n",
    "    GLR network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width=36, cuda=False):\n",
    "        super(GTV, self).__init__()\n",
    "        self.cnnf = cnnf()\n",
    "        self.cnny = cnny()\n",
    "        self.cnnp = cnnp()\n",
    "        self.wt = width\n",
    "    \n",
    "        if cuda:\n",
    "            self.cnnf.cuda()\n",
    "            \n",
    "        self.dtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "        self.cnnf.apply(weights_init_normal)\n",
    "        \n",
    "\n",
    "    def forward(self, xf):\n",
    "        img_dim = self.wt\n",
    "        \n",
    "        E = self.cnnf.forward(xf).squeeze(0)\n",
    "        Y = self.cnny.forward(xf).squeeze(0)\n",
    "        _lambda = self.cnnp.forward(xf.view(xf.shape[0], xf.shape[1], img_dim**2))\n",
    "\n",
    "        A = laplacian_construction(\n",
    "            width=img_dim, F=E.view(E.shape[0], E.shape[1], img_dim ** 2)\n",
    "        )\n",
    "        x = Y.view(Y.shape[0],Y.shape[1], img_dim**2)\n",
    "        W = A.view(A.shape[0], A.shape[1], img_dim**2)\n",
    "        \n",
    "        out = proximal_gradient_descent(x=x, y=x, w=W, eta=_lambda)\n",
    "        \n",
    "        return out.view(xf.shape[0], 3, img_dim, img_dim)\n",
    "\n",
    "    def predict(self, xf):\n",
    "        pass\n",
    "\n",
    "def proximal_gradient_descent(x, y, w, eta=1): \n",
    "    \n",
    "    grad = eta * (2*x - 2*y)\n",
    "    w =  w.sum(axis=2).unsqueeze(1)\n",
    "    w = w.repeat(1, 3, 1)\n",
    "    x = x - grad\n",
    "    xhat = prox_gtv(w, x, eta)\n",
    "\n",
    "    return xhat\n",
    "\n",
    "def prox_gtv(w, v, eta=1):\n",
    "#     with torch.no_grad():\n",
    "    \n",
    "#     print(masks.shape)\n",
    "#     v[ ~masks] = 0\n",
    "#     with torch.no_grad():\n",
    "#         V = v[masks] - torch.mul(torch.mul(eta,  w[masks]) ,torch.sign(v[masks]))\n",
    "    masks = (w>0).type(dtype)\n",
    "    v = v- masks*eta*w*torch.sign(v)\n",
    "    masks = (w<=0).type(dtype)\n",
    "    v = v -masks*v\n",
    "#     a = torch.mul(eta,  w[masks])\n",
    "#     print(w[masks].shape, eta.shape, v.shape, a.shape)\n",
    "#     v[masks] = V\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA:  False\n",
      "Wed Mar  4 03:38:02 2020 [1] loss: 460.757, time elapsed: 11.662750005722046\n",
      "Wed Mar  4 03:38:15 2020 [2] loss: 329.780, time elapsed: 24.43459463119507\n",
      "Wed Mar  4 03:38:27 2020 [3] loss: 319.323, time elapsed: 36.11355471611023\n",
      "Wed Mar  4 03:38:39 2020 [4] loss: 316.409, time elapsed: 48.36962080001831\n",
      "Wed Mar  4 03:38:51 2020 [5] loss: 307.043, time elapsed: 60.55535626411438\n",
      "save @ epoch  5\n",
      "Wed Mar  4 03:39:03 2020 [6] loss: 301.104, time elapsed: 72.77688074111938\n",
      "Wed Mar  4 03:39:16 2020 [7] loss: 292.008, time elapsed: 85.05217862129211\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "print(\"CUDA: \", cuda)\n",
    "if cuda:\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    dtype = torch.FloatTensor\n",
    "    \n",
    "batch_size = 100\n",
    "width = 36\n",
    "\n",
    "lr = 2e-5\n",
    "total_epoch = 100 \n",
    "DST = \"./\"\n",
    "PATH = os.path.join(DST, \"DGLR.pkl\")\n",
    "\n",
    "dataset = RENOIR_Dataset(\n",
    "    img_dir=os.path.join('batch'),\n",
    "    transform=transforms.Compose([standardize(), ToTensor()]),\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=batch_size, shuffle=True, pin_memory=True\n",
    ")\n",
    "gtv = GTV(width=36, cuda=cuda)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.AdamW(gtv.parameters(), lr=lr)\n",
    "# optimizer = optim.SGD(gtv.parameters(), lr=0.05)\n",
    "hist = list()\n",
    "tstart = time.time()\n",
    "for epoch in range(total_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):  # start index at 0\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        labels = data[\"rimg\"].float().type(dtype)\n",
    "        inputs = data[\"nimg\"].float().type(dtype)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = gtv(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    print(\n",
    "        time.ctime(),\n",
    "        \"[{0}] loss: {1:.3f}, time elapsed: {2}\".format(\n",
    "            epoch + 1, running_loss / (i + 1), time.time() - tstart\n",
    "        ),\n",
    "    )\n",
    "    hist.append(running_loss / (i + 1))\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(\"save @ epoch \", epoch + 1)\n",
    "        torch.save(gtv.state_dict(), PATH)\n",
    "\n",
    "torch.save(glr.state_dict(), PATH)\n",
    "print(\"Total running time: {0:.3f}\".format(time.time() - tstart))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise(inp, gtv, argref):\n",
    "    \n",
    "    try:\n",
    "        from skimage.metrics import structural_similarity as compare_ssim\n",
    "    except Exception:\n",
    "        from skimage.measure import compare_ssim\n",
    "\n",
    "\n",
    "    sample = cv2.imread(inp)\n",
    "    shape = sample.shape\n",
    "    \n",
    "    width = 324\n",
    "    sample = cv2.resize(sample, (width, width))\n",
    "    sample = cv2.cvtColor(sample, cv2.COLOR_BGR2RGB)\n",
    "    sample = sample.transpose((2, 0, 1))\n",
    "    sample = torch.from_numpy(sample)\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    \n",
    "    device = torch.device(\"cuda\") if cuda else torch.device(\"cpu\")\n",
    "    \n",
    "    dtype = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    psnrs = list()\n",
    "    if argref:\n",
    "        ref = cv2.imread(argref)\n",
    "        if ref.shape[0] != width or ref.shape[1] != width:\n",
    "            ref = cv2.resize(ref, (width, width))\n",
    "#         print(ref.shape)\n",
    "        ref = cv2.cvtColor(ref, cv2.COLOR_BGR2RGB)\n",
    "        tref = ref.copy()\n",
    "        ref = ref.transpose((2, 0, 1))\n",
    "        ref = torch.from_numpy(ref)\n",
    "\n",
    "    tstart = time.time()\n",
    "    T1 = sample\n",
    "    if argref:\n",
    "        T1r = ref\n",
    "#         print(T1r.shape, T1.shape)\n",
    "    else:\n",
    "        print(T1.shape)\n",
    "    m = T1.shape[-1]\n",
    "    dummy = np.zeros(shape=(3, T1.shape[-1], T1.shape[-2]))\n",
    "    T2 = (\n",
    "        torch.from_numpy(T1.detach().numpy().transpose(1, 2, 0))\n",
    "        .unfold(0, 36, 36)\n",
    "        .unfold(1, 36, 36)\n",
    "    ).type(dtype)\n",
    "    if argref:\n",
    "        T2r = (\n",
    "            torch.from_numpy(T1r.detach().numpy().transpose(1, 2, 0))\n",
    "            .unfold(0, 36, 36)\n",
    "            .unfold(1, 36, 36)\n",
    "        )\n",
    "\n",
    "    s2 = int(T2.shape[-1])\n",
    "\n",
    "    for ii, i in enumerate(range(T2.shape[1])):\n",
    "        P = gtv.forward(T2[i, :, :, :, :].float())\n",
    "\n",
    "        if cuda:\n",
    "            P = P.cpu()\n",
    "        if argref:\n",
    "            img1 = T2r[i, :, :, :, :].float()\n",
    "            img2 = P\n",
    "            print(img1.shape, img2.shape)\n",
    "            psnrs.append(cv2.PSNR(img1.detach().numpy(), img2.detach().numpy()))\n",
    "\n",
    "        print(\"\\r{0}, {1}/{2}\".format(P.shape, ii + 1, P.shape[0]), end=\" \")\n",
    "        for b, j in enumerate(range(0, m, s2)):\n",
    "            dummy[:, (i * s2) : (i * s2 + s2), j : (j + s2)] = P[b].detach().numpy()\n",
    "    print(\"\\nPrediction time: \", time.time() - tstart)\n",
    "    if argref:\n",
    "        print(\"PSNR: \", np.mean(np.array(psnrs)))\n",
    "\n",
    "    ds = np.array(dummy).copy()\n",
    "    new_d = list()\n",
    "    for d in ds:\n",
    "        _d = (d - d.min()) * (1 / (d.max() - d.min()))\n",
    "        new_d.append(_d)\n",
    "    d = np.array(new_d).transpose(1, 2, 0)\n",
    "    if 0:\n",
    "        opath = args.output\n",
    "    else:\n",
    "        filename = inp.split(\"/\")[-1]\n",
    "        opath = \"./{0}_{1}\".format(\"denoised\", filename)\n",
    "    plt.imsave(opath, d)\n",
    "    if argref:\n",
    "        d = cv2.imread(opath)\n",
    "        d = cv2.cvtColor(d, cv2.COLOR_BGR2RGB)\n",
    "        (score, diff) = compare_ssim(tref, d, full=True, multichannel=True)\n",
    "        print(\"SSIM: \", score)\n",
    "    print(\"Saved \", opath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 1/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 2/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 3/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 4/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 5/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 6/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 7/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 8/9 torch.Size([9, 3, 36, 36]) torch.Size([9, 3, 36, 36])\n",
      "torch.Size([9, 3, 36, 36]), 9/9 \n",
      "Prediction time:  4.894117832183838\n",
      "PSNR:  25.54222234674288\n",
      "SSIM:  0.6520306342364103\n",
      "Saved  ./denoised_2_n.bmp\n"
     ]
    }
   ],
   "source": [
    "inp = '/home/huy/eecs5323/eecs5323_project/dataset/resize/train/noisy/2_n.bmp'\n",
    "argref = '/home/huy/eecs5323/eecs5323_project/dataset/resize/train/ref/2_r.bmp'\n",
    "denoise(inp, gtv, argref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.46509014289845735, 22.530924115467176)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    from skimage.metrics import structural_similarity as compare_ssim\n",
    "except Exception:\n",
    "    from skimage.measure import compare_ssim\n",
    "\n",
    "img1 = cv2.imread(inp)\n",
    "img2 = cv2.imread(argref)\n",
    "(score, diff) = compare_ssim(img1, img2, full=True, multichannel=True)\n",
    "score, cv2.PSNR(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
